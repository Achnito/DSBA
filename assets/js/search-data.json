{
  
    
        "post0": {
            "title": "Data Science for Business Applications",
            "content": "Pokemon - Assignment 1 . %%shell jupyter nbconvert --to html /content/M1_Assignment.ipynb . [NbConvertApp] Converting notebook /content/M1_Assignment.ipynb to html [NbConvertApp] Writing 1689830 bytes to /content/M1_Assignment.html . . from IPython.display import HTML HTML(&#39;&#39;&#39;&lt;script&gt; code_show=true; function code_toggle() { if (code_show){ $(&#39;div.input&#39;).hide(); } else { $(&#39;div.input&#39;).show(); } code_show = !code_show } $( document ).ready(code_toggle); &lt;/script&gt; &lt;form action=&quot;javascript:code_toggle()&quot;&gt;&lt;input type=&quot;submit&quot; value=&quot;Click here to toggle on/off the raw code.&quot;&gt;&lt;/form&gt;&#39;&#39;&#39;) . Initialize . Imports . import pandas as pd #DataFrame awesomeness import numpy as np #Math library import matplotlib.pyplot as plt #Statistics Visualization Library import seaborn as sns #2nd Gen StatsViz! import plotly.express as px #Visualization tool! import sklearn.metrics as mt #Metrics - measure model performance #Data Preprocessing tools from sklearn.preprocessing import StandardScaler #transformsdata to having mean=0 &amp; std=1 from sklearn.preprocessing import MinMaxScaler #Dimensionality Reduction tools from sklearn.decomposition import PCA from sklearn.decomposition import nmf import umap as umap #Get ready to plot those umaps!! !pip install umap-learn[plot] import umap.plot #Something else I forgot sns.set(color_codes=True, rc={&#39;figure.figsize&#39;:(10,8)}) . Requirement already satisfied: umap-learn[plot] in /usr/local/lib/python3.6/dist-packages (0.4.6) Requirement already satisfied: scikit-learn&gt;=0.20 in /usr/local/lib/python3.6/dist-packages (from umap-learn[plot]) (0.22.2.post1) Requirement already satisfied: scipy&gt;=1.3.1 in /usr/local/lib/python3.6/dist-packages (from umap-learn[plot]) (1.4.1) Requirement already satisfied: numba!=0.47,&gt;=0.46 in /usr/local/lib/python3.6/dist-packages (from umap-learn[plot]) (0.48.0) Requirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.6/dist-packages (from umap-learn[plot]) (1.18.5) Requirement already satisfied: matplotlib; extra == &#34;plot&#34; in /usr/local/lib/python3.6/dist-packages (from umap-learn[plot]) (3.2.2) Requirement already satisfied: datashader; extra == &#34;plot&#34; in /usr/local/lib/python3.6/dist-packages (from umap-learn[plot]) (0.11.1) Requirement already satisfied: pandas; extra == &#34;plot&#34; in /usr/local/lib/python3.6/dist-packages (from umap-learn[plot]) (1.1.3) Requirement already satisfied: colorcet; extra == &#34;plot&#34; in /usr/local/lib/python3.6/dist-packages (from umap-learn[plot]) (2.0.2) Requirement already satisfied: bokeh; extra == &#34;plot&#34; in /usr/local/lib/python3.6/dist-packages (from umap-learn[plot]) (2.1.1) Requirement already satisfied: holoviews; extra == &#34;plot&#34; in /usr/local/lib/python3.6/dist-packages (from umap-learn[plot]) (1.13.4) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn&gt;=0.20-&gt;umap-learn[plot]) (0.17.0) Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba!=0.47,&gt;=0.46-&gt;umap-learn[plot]) (50.3.0) Requirement already satisfied: llvmlite&lt;0.32.0,&gt;=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba!=0.47,&gt;=0.46-&gt;umap-learn[plot]) (0.31.0) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (0.10.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (2.4.7) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (1.2.0) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (2.8.1) Requirement already satisfied: dask[complete]&gt;=0.18.0 in /usr/local/lib/python3.6/dist-packages (from datashader; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (2.12.0) Requirement already satisfied: param&gt;=1.6.0 in /usr/local/lib/python3.6/dist-packages (from datashader; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (1.9.3) Requirement already satisfied: datashape&gt;=0.5.1 in /usr/local/lib/python3.6/dist-packages (from datashader; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (0.5.2) Requirement already satisfied: pillow&gt;=3.1.1 in /usr/local/lib/python3.6/dist-packages (from datashader; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (7.0.0) Requirement already satisfied: toolz&gt;=0.7.4 in /usr/local/lib/python3.6/dist-packages (from datashader; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (0.11.1) Requirement already satisfied: xarray&gt;=0.9.6 in /usr/local/lib/python3.6/dist-packages (from datashader; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (0.15.1) Requirement already satisfied: pyct[cmd]==0.4.6 in /usr/local/lib/python3.6/dist-packages (from datashader; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (0.4.6) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (2018.9) Requirement already satisfied: Jinja2&gt;=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (2.11.2) Requirement already satisfied: packaging&gt;=16.8 in /usr/local/lib/python3.6/dist-packages (from bokeh; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (20.4) Requirement already satisfied: typing-extensions&gt;=3.7.4 in /usr/local/lib/python3.6/dist-packages (from bokeh; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (3.7.4.3) Requirement already satisfied: PyYAML&gt;=3.10 in /usr/local/lib/python3.6/dist-packages (from bokeh; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (3.13) Requirement already satisfied: tornado&gt;=5.1 in /usr/local/lib/python3.6/dist-packages (from bokeh; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (5.1.1) Requirement already satisfied: panel&gt;=0.8.0 in /usr/local/lib/python3.6/dist-packages (from holoviews; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (0.9.7) Requirement already satisfied: pyviz-comms&gt;=0.7.3 in /usr/local/lib/python3.6/dist-packages (from holoviews; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (0.7.6) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler&gt;=0.10-&gt;matplotlib; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (1.15.0) Requirement already satisfied: fsspec&gt;=0.6.0; extra == &#34;complete&#34; in /usr/local/lib/python3.6/dist-packages (from dask[complete]&gt;=0.18.0-&gt;datashader; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (0.8.4) Requirement already satisfied: distributed&gt;=2.0; extra == &#34;complete&#34; in /usr/local/lib/python3.6/dist-packages (from dask[complete]&gt;=0.18.0-&gt;datashader; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (2.30.0) Requirement already satisfied: cloudpickle&gt;=0.2.1; extra == &#34;complete&#34; in /usr/local/lib/python3.6/dist-packages (from dask[complete]&gt;=0.18.0-&gt;datashader; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (1.3.0) Requirement already satisfied: partd&gt;=0.3.10; extra == &#34;complete&#34; in /usr/local/lib/python3.6/dist-packages (from dask[complete]&gt;=0.18.0-&gt;datashader; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (1.1.0) Requirement already satisfied: multipledispatch&gt;=0.4.7 in /usr/local/lib/python3.6/dist-packages (from datashape&gt;=0.5.1-&gt;datashader; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (0.6.0) Requirement already satisfied: requests; extra == &#34;cmd&#34; in /usr/local/lib/python3.6/dist-packages (from pyct[cmd]==0.4.6-&gt;datashader; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (2.23.0) Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2&gt;=2.7-&gt;bokeh; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (1.1.1) Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from panel&gt;=0.8.0-&gt;holoviews; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (4.41.1) Requirement already satisfied: markdown in /usr/local/lib/python3.6/dist-packages (from panel&gt;=0.8.0-&gt;holoviews; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (3.3.2) Requirement already satisfied: psutil&gt;=5.0 in /usr/local/lib/python3.6/dist-packages (from distributed&gt;=2.0; extra == &#34;complete&#34;-&gt;dask[complete]&gt;=0.18.0-&gt;datashader; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (5.4.8) Requirement already satisfied: zict&gt;=0.1.3 in /usr/local/lib/python3.6/dist-packages (from distributed&gt;=2.0; extra == &#34;complete&#34;-&gt;dask[complete]&gt;=0.18.0-&gt;datashader; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (2.0.0) Requirement already satisfied: tblib&gt;=1.6.0 in /usr/local/lib/python3.6/dist-packages (from distributed&gt;=2.0; extra == &#34;complete&#34;-&gt;dask[complete]&gt;=0.18.0-&gt;datashader; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (1.7.0) Requirement already satisfied: msgpack&gt;=0.6.0 in /usr/local/lib/python3.6/dist-packages (from distributed&gt;=2.0; extra == &#34;complete&#34;-&gt;dask[complete]&gt;=0.18.0-&gt;datashader; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (1.0.0) Requirement already satisfied: contextvars; python_version &lt; &#34;3.7&#34; in /usr/local/lib/python3.6/dist-packages (from distributed&gt;=2.0; extra == &#34;complete&#34;-&gt;dask[complete]&gt;=0.18.0-&gt;datashader; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (2.4) Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.6/dist-packages (from distributed&gt;=2.0; extra == &#34;complete&#34;-&gt;dask[complete]&gt;=0.18.0-&gt;datashader; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (2.2.2) Requirement already satisfied: click&gt;=6.6 in /usr/local/lib/python3.6/dist-packages (from distributed&gt;=2.0; extra == &#34;complete&#34;-&gt;dask[complete]&gt;=0.18.0-&gt;datashader; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (7.1.2) Requirement already satisfied: locket in /usr/local/lib/python3.6/dist-packages (from partd&gt;=0.3.10; extra == &#34;complete&#34;-&gt;dask[complete]&gt;=0.18.0-&gt;datashader; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (0.2.0) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests; extra == &#34;cmd&#34;-&gt;pyct[cmd]==0.4.6-&gt;datashader; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (1.24.3) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests; extra == &#34;cmd&#34;-&gt;pyct[cmd]==0.4.6-&gt;datashader; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (2020.6.20) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests; extra == &#34;cmd&#34;-&gt;pyct[cmd]==0.4.6-&gt;datashader; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests; extra == &#34;cmd&#34;-&gt;pyct[cmd]==0.4.6-&gt;datashader; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (3.0.4) Requirement already satisfied: importlib-metadata; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.6/dist-packages (from markdown-&gt;panel&gt;=0.8.0-&gt;holoviews; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (2.0.0) Requirement already satisfied: heapdict in /usr/local/lib/python3.6/dist-packages (from zict&gt;=0.1.3-&gt;distributed&gt;=2.0; extra == &#34;complete&#34;-&gt;dask[complete]&gt;=0.18.0-&gt;datashader; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (1.0.1) Requirement already satisfied: immutables&gt;=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars; python_version &lt; &#34;3.7&#34;-&gt;distributed&gt;=2.0; extra == &#34;complete&#34;-&gt;dask[complete]&gt;=0.18.0-&gt;datashader; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (0.14) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;markdown-&gt;panel&gt;=0.8.0-&gt;holoviews; extra == &#34;plot&#34;-&gt;umap-learn[plot]) (3.3.1) . Get Data . pokemon_URL = &#39;https://sds-aau.github.io/SDS-master/00_data/pokemon.csv&#39; . pokemon = pd.read_csv(pokemon_URL) . Task 1 - Data Preprocessing . Give a brief overview of data, what variables are there, how are the variables scaled and variation of the data columns. . pokemon.head() . Number Name Type1 Type2 Total HitPoints Attack Defense SpecialAttack SpecialDefense Speed Generation Legendary . 0 1 | Bulbasaur | Grass | Poison | 318 | 45 | 49 | 49 | 65 | 65 | 45 | 1 | False | . 1 2 | Ivysaur | Grass | Poison | 405 | 60 | 62 | 63 | 80 | 80 | 60 | 1 | False | . 2 3 | Venusaur | Grass | Poison | 525 | 80 | 82 | 83 | 100 | 100 | 80 | 1 | False | . 3 3 | VenusaurMega Venusaur | Grass | Poison | 625 | 80 | 100 | 123 | 122 | 120 | 80 | 1 | False | . 4 4 | Charmander | Fire | NaN | 309 | 39 | 52 | 43 | 60 | 50 | 65 | 1 | False | . pokemon.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 800 entries, 0 to 799 Data columns (total 13 columns): # Column Non-Null Count Dtype -- -- 0 Number 800 non-null int64 1 Name 800 non-null object 2 Type1 800 non-null object 3 Type2 414 non-null object 4 Total 800 non-null int64 5 HitPoints 800 non-null int64 6 Attack 800 non-null int64 7 Defense 800 non-null int64 8 SpecialAttack 800 non-null int64 9 SpecialDefense 800 non-null int64 10 Speed 800 non-null int64 11 Generation 800 non-null int64 12 Legendary 800 non-null bool dtypes: bool(1), int64(9), object(3) memory usage: 75.9+ KB . We see that the data lists 800 Pokemons divided by 13 columns. . Data Types: . string: Name | categorical: Type1, Type2, Generation and Legendary | numerical: all other | . Note that some values in Type2 are missing. This indicates pokemon that does not have a second type. . Even though Generation and legendary data are numerical, we will work with them as if they were categorical. . Some change should be added: . Replace missing values in Type2 with &#39;None&#39; | Change Type1, Type2, Generation, and Legendary to category data. | Edit Name: Total and Legendary should be renamed to Total_Stats and Legendary_Status to avoid confusion. | Fixing data before further investigation . pokemon.Type2.fillna(&#39;None&#39;,inplace=True) #Set variables as categorical pokemon[&#39;Type1&#39;] = pokemon.Type1.astype(&#39;category&#39;) pokemon[&#39;Type2&#39;] = pokemon.Type2.astype(&#39;category&#39;) pokemon[&#39;Generation&#39;] = pokemon.Generation.astype(&#39;category&#39;) pokemon[&#39;Legendary&#39;] = pokemon.Legendary.astype(&#39;category&#39;) #Change name of column pokemon.rename(columns={&#39;Total&#39;: &#39;Total_Stats&#39;,&#39;Legendary&#39;:&#39;Legendary_Status&#39;}, inplace=True) . Inspect the data further . type1 = pokemon[&#39;Type1&#39;].value_counts() . type1 . Water 112 Normal 98 Grass 70 Bug 69 Psychic 57 Fire 52 Rock 44 Electric 44 Ground 32 Dragon 32 Ghost 32 Dark 31 Poison 28 Steel 27 Fighting 27 Ice 24 Fairy 17 Flying 4 Name: Type1, dtype: int64 . type2 = pokemon[&#39;Type2&#39;].value_counts() . type2 . None 386 Flying 97 Ground 35 Poison 34 Psychic 33 Fighting 26 Grass 25 Fairy 23 Steel 22 Dark 20 Dragon 18 Water 14 Ghost 14 Ice 14 Rock 14 Fire 12 Electric 6 Normal 4 Bug 3 Name: Type2, dtype: int64 . type_total = (type1+type2).sort_values(ascending=False) . type_total . Water 126.0 Normal 102.0 Flying 101.0 Grass 95.0 Psychic 90.0 Bug 72.0 Ground 67.0 Fire 64.0 Poison 62.0 Rock 58.0 Fighting 53.0 Dark 51.0 Electric 50.0 Dragon 50.0 Steel 49.0 Ghost 46.0 Fairy 40.0 Ice 38.0 None NaN dtype: float64 . pokemon[&#39;Legendary_Status&#39;].value_counts() . False 735 True 65 Name: Legendary_Status, dtype: int64 . pokemon.describe() . Number Total_Stats HitPoints Attack Defense SpecialAttack SpecialDefense Speed . count 800.000000 | 800.00000 | 800.000000 | 800.000000 | 800.000000 | 800.000000 | 800.000000 | 800.000000 | . mean 362.813750 | 435.10250 | 69.258750 | 79.001250 | 73.842500 | 72.820000 | 71.902500 | 68.277500 | . std 208.343798 | 119.96304 | 25.534669 | 32.457366 | 31.183501 | 32.722294 | 27.828916 | 29.060474 | . min 1.000000 | 180.00000 | 1.000000 | 5.000000 | 5.000000 | 10.000000 | 20.000000 | 5.000000 | . 25% 184.750000 | 330.00000 | 50.000000 | 55.000000 | 50.000000 | 49.750000 | 50.000000 | 45.000000 | . 50% 364.500000 | 450.00000 | 65.000000 | 75.000000 | 70.000000 | 65.000000 | 70.000000 | 65.000000 | . 75% 539.250000 | 515.00000 | 80.000000 | 100.000000 | 90.000000 | 95.000000 | 90.000000 | 90.000000 | . max 721.000000 | 780.00000 | 255.000000 | 190.000000 | 230.000000 | 194.000000 | 230.000000 | 180.000000 | . pokemon[pokemon.Legendary_Status == True].describe() . Number Total_Stats HitPoints Attack Defense SpecialAttack SpecialDefense Speed . count 65.000000 | 65.000000 | 65.000000 | 65.000000 | 65.000000 | 65.000000 | 65.000000 | 65.000000 | . mean 470.215385 | 637.384615 | 92.738462 | 116.676923 | 99.661538 | 122.184615 | 105.938462 | 100.184615 | . std 173.651095 | 60.937389 | 21.722164 | 30.348037 | 28.255131 | 31.104608 | 28.827004 | 22.952323 | . min 144.000000 | 580.000000 | 50.000000 | 50.000000 | 20.000000 | 50.000000 | 20.000000 | 50.000000 | . 25% 381.000000 | 580.000000 | 80.000000 | 100.000000 | 90.000000 | 100.000000 | 90.000000 | 90.000000 | . 50% 483.000000 | 600.000000 | 91.000000 | 110.000000 | 100.000000 | 120.000000 | 100.000000 | 100.000000 | . 75% 642.000000 | 680.000000 | 105.000000 | 131.000000 | 115.000000 | 150.000000 | 120.000000 | 110.000000 | . max 721.000000 | 780.000000 | 150.000000 | 190.000000 | 200.000000 | 194.000000 | 200.000000 | 180.000000 | . pokemon[pokemon.Legendary_Status == False].describe() . Number Total_Stats HitPoints Attack Defense SpecialAttack SpecialDefense Speed . count 735.000000 | 735.000000 | 735.000000 | 735.000000 | 735.000000 | 735.000000 | 735.000000 | 735.000000 | . mean 353.315646 | 417.213605 | 67.182313 | 75.669388 | 71.559184 | 68.454422 | 68.892517 | 65.455782 | . std 208.590419 | 106.760417 | 24.808849 | 30.490153 | 30.408194 | 29.091705 | 25.669310 | 27.843038 | . min 1.000000 | 180.000000 | 1.000000 | 5.000000 | 5.000000 | 10.000000 | 20.000000 | 5.000000 | . 25% 175.500000 | 324.000000 | 50.000000 | 54.500000 | 50.000000 | 45.000000 | 50.000000 | 45.000000 | . 50% 346.000000 | 425.000000 | 65.000000 | 72.000000 | 66.000000 | 65.000000 | 65.000000 | 64.000000 | . 75% 533.500000 | 498.000000 | 79.500000 | 95.000000 | 85.000000 | 85.000000 | 85.000000 | 85.000000 | . max 715.000000 | 700.000000 | 255.000000 | 185.000000 | 230.000000 | 175.000000 | 230.000000 | 160.000000 | . pokemon.sort_values(by=&#39;Total_Stats&#39;, ascending=False).head(10) . Number Name Type1 Type2 Total_Stats HitPoints Attack Defense SpecialAttack SpecialDefense Speed Generation Legendary_Status . 426 384 | RayquazaMega Rayquaza | Dragon | Flying | 780 | 105 | 180 | 100 | 180 | 100 | 115 | 3 | True | . 164 150 | MewtwoMega Mewtwo Y | Psychic | None | 780 | 106 | 150 | 70 | 194 | 120 | 140 | 1 | True | . 163 150 | MewtwoMega Mewtwo X | Psychic | Fighting | 780 | 106 | 190 | 100 | 154 | 100 | 130 | 1 | True | . 422 382 | KyogrePrimal Kyogre | Water | None | 770 | 100 | 150 | 90 | 180 | 160 | 90 | 3 | True | . 424 383 | GroudonPrimal Groudon | Ground | Fire | 770 | 100 | 180 | 160 | 150 | 90 | 90 | 3 | True | . 552 493 | Arceus | Normal | None | 720 | 120 | 120 | 120 | 120 | 120 | 120 | 4 | True | . 712 646 | KyuremWhite Kyurem | Dragon | Ice | 700 | 125 | 120 | 90 | 170 | 100 | 95 | 5 | True | . 711 646 | KyuremBlack Kyurem | Dragon | Ice | 700 | 125 | 170 | 100 | 120 | 90 | 95 | 5 | True | . 409 373 | SalamenceMega Salamence | Dragon | Flying | 700 | 95 | 145 | 130 | 120 | 90 | 120 | 3 | False | . 413 376 | MetagrossMega Metagross | Steel | Psychic | 700 | 80 | 145 | 150 | 105 | 110 | 110 | 3 | False | . pokemon.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 800 entries, 0 to 799 Data columns (total 13 columns): # Column Non-Null Count Dtype -- -- 0 Number 800 non-null int64 1 Name 800 non-null object 2 Type1 800 non-null category 3 Type2 800 non-null category 4 Total_Stats 800 non-null int64 5 HitPoints 800 non-null int64 6 Attack 800 non-null int64 7 Defense 800 non-null int64 8 SpecialAttack 800 non-null int64 9 SpecialDefense 800 non-null int64 10 Speed 800 non-null int64 11 Generation 800 non-null category 12 Legendary_Status 800 non-null category dtypes: category(4), int64(8), object(1) memory usage: 61.3+ KB . After some data preprocessing, we now have: . 4 categorical variables (Type1, Type2, Generation, and Legendary) | 8 numerical variables (Number, 6 different stats, and total stats) | . We find that all numerical values are normally distributed (mean and median are almost identical). . Types of pokemons . Water is the most common type , whilst Ice is the least common type. Water is seen 126 times, and Ice is only seen 38 times in the 800 observed pokemons. . Allmost half (386) of the pokemons do not have a Type2. . 65 pokemons are Legendary. . Pokemon Stats . The pokemon with the least amount of stats only has 180 total stat, while the pokemon with the most has 780. . Stats generally range from 1 to 255 and with a standard dev. of ~25-32 . Legendary Pokemons are generally higher in all stats. However, some normal pokemons are included in the top 10 most powerful pokemon (Total_Stats), just 80 points short from most powerful pokemon. . Side note: The most powerful normal pokemons tend to have &quot;Mega&quot; in their name. . Visualization of Pokemon Stats . pokemon_num = pokemon._get_numeric_data() #select all numeric values! . pokemon_num . Number Total_Stats HitPoints Attack Defense SpecialAttack SpecialDefense Speed . 0 1 | 318 | 45 | 49 | 49 | 65 | 65 | 45 | . 1 2 | 405 | 60 | 62 | 63 | 80 | 80 | 60 | . 2 3 | 525 | 80 | 82 | 83 | 100 | 100 | 80 | . 3 3 | 625 | 80 | 100 | 123 | 122 | 120 | 80 | . 4 4 | 309 | 39 | 52 | 43 | 60 | 50 | 65 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 795 719 | 600 | 50 | 100 | 150 | 100 | 150 | 50 | . 796 719 | 700 | 50 | 160 | 110 | 160 | 110 | 110 | . 797 720 | 600 | 80 | 110 | 60 | 150 | 130 | 70 | . 798 720 | 680 | 80 | 160 | 60 | 170 | 130 | 80 | . 799 721 | 600 | 80 | 110 | 120 | 130 | 90 | 70 | . 800 rows × 8 columns . sns.pairplot( pokemon, vars = pokemon_num, hue = &#39;Legendary_Status&#39;, kind = &#39;reg&#39;, corner = True, plot_kws = {&#39;line_kws&#39;:{&#39;color&#39;:&#39;black&#39;}} ) . &lt;seaborn.axisgrid.PairGrid at 0x7f3dd9053f60&gt; . Task 2 - Scaling &amp; PCA Analysis . Execute a PCA analysis on all numerical variables in the dataset. Hint: Don&#39;t forget to scale them first. Use 4 components. What is the individuel and cumulative explained variance? . . Scale that bish! . Since the data appears to be normally distributed we apply the StandardScaler . scaler = StandardScaler() . scaled_pokemon = scaler.fit_transform(pokemon_num) . for i in range(8): #We use 8 here since there are 8 features (columns) sns.distplot(scaled_pokemon[:,i],hist=False) . /usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). /usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). /usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). /usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). /usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). /usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). /usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). /usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). . Perform PCA on that nig! . pca = PCA(n_components=4) . pca_pokemon = pca.fit_transform(scaled_pokemon) . pca_shape = pca_pokemon.shape # 800 rows, 4 columns - As we wanted :-) . Investigate Principal Component Variance! . Now that we have performed a PCA reduction of the pokemon data, let&#39;s see how the components perform! . pca_variance = pca.explained_variance_ratio_ #lists the variance ratio of each component (how much of the data&#39;s variance does the component explain?) . pca_cumVar =np.cumsum(pca_variance) #calculate the cumulative variance ratio of the components . pd.DataFrame({&#39;variance ratio&#39;:pca_variance, &#39;cumulative variance ratio&#39;:pca_cumVar}) . variance ratio cumulative variance ratio . 0 0.466172 | 0.466172 | . 1 0.139397 | 0.605569 | . 2 0.120448 | 0.726018 | . 3 0.097013 | 0.823031 | . The individually explained variance for the principal components are 47, 14, 12 and 9%. Overall explained variance is 82%! . Visualize PCA on Legendary Status . Note, that we have taken away all categorical data. Let&#39;s see if the PCA can distinguish between pokemon generation and legendary status, using the provided data . Legendary! . . pca_labels = { str(i): f&quot;PC {i+1} ({var:.1f}%)&quot; for i, var in enumerate(pca_variance* 100) } . fig = px.scatter_matrix( pca_pokemon, # data to plot labels=pca_labels, dimensions=range(4), color=pokemon[&quot;Legendary_Status&quot;], title = &#39;PCA on numeric types, colored by Legendary Status&#39; #found it interesting to color by Legendary Status, as these may introduce different stats ) fig.update_traces(diagonal_visible=False) fig.show() . . . sns.scatterplot(pca_pokemon[:,0], pca_pokemon[:,1], hue = pokemon[&#39;Legendary_Status&#39;]) . /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3dd207dc50&gt; . PC1 is generally good at explaining the legendary status of a pokemon. However, some overlapping occurs! . Generation! . fig = px.scatter_matrix( pca_pokemon, # data to plot labels=pca_labels, dimensions=range(4), #We only have 4 components color=pokemon[&quot;Generation&quot;], title = &#39;PCA on numeric types, colored by Generation&#39; #found it interesting to color by Generation, as these may introduce different stats ) fig.update_traces(diagonal_visible=False) fig.show() . . . sns.scatterplot(pca_pokemon[:,2], pca_pokemon[:,1], hue = pokemon[&#39;Generation&#39;]) . /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3dd2071390&gt; . PC2 is surprisingly good at finding the hidden patterns of the generations in combination with PC3. . Overall PCA performs really well on the pokemon data! . Task 3 - UMAP / NMF . Use a different dimensionality reduction method (eg. UMAP/NMF) – do the findings differ? . Sidenote: UMAP (Uniform Manifold Approximation and Projection) and NMF (Non-negative matrix factorization) are non-linear algorithms. NMF is mostly used for text and imagery while UMAP is mostly used for visualization purposes. . In the case of the pokemon data, we find that most have a LINEAR relationship. Thus we can expect PCA to outperform both UMAP and NMF. . . UMAP . Let&#39;s try to UMAP a picachu! . umap_model = umap.UMAP() . umap_pokemon = umap_model.fit_transform(scaled_pokemon) . UMAP_shape = umap_pokemon.shape #Woah only 2 components! . viz that Biz! . to_plot = umap_model.fit(scaled_pokemon) . sns.scatterplot(umap_pokemon[:,0],umap_pokemon[:,1], hue= pokemon[&#39;Legendary_Status&#39;]) . /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3dd20d3b70&gt; . umap.plot.points( to_plot, #data we want to plot theme=&#39;fire&#39; ) . *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3dd136f3c8&gt; . hover_data = pd.DataFrame({&#39;index&#39;: np.arange(800), &#39;label&#39;:pokemon.Generation[:800]}) hover_data[&#39;item&#39;] = hover_data.label.map( { 1:&#39;1st Gen&#39;, 2:&#39;2nd Gen&#39;, 3:&#39;3rd Gen&#39;, 4:&#39;4th Gen&#39;, 5:&#39;5th Gen&#39;, 6:&#39;6th Gen&#39; } ) . hover_data . index label item . 0 0 | 1 | 1st Gen | . 1 1 | 1 | 1st Gen | . 2 2 | 1 | 1st Gen | . 3 3 | 1 | 1st Gen | . 4 4 | 1 | 1st Gen | . ... ... | ... | ... | . 795 795 | 6 | 6th Gen | . 796 796 | 6 | 6th Gen | . 797 797 | 6 | 6th Gen | . 798 798 | 6 | 6th Gen | . 799 799 | 6 | 6th Gen | . 800 rows × 3 columns . p = umap.plot.interactive(to_plot, labels=pokemon.Generation, hover_data=hover_data, point_size=2) umap.plot.show(p) . sns.scatterplot(umap_pokemon[:,0],umap_pokemon[:,1], hue= pokemon[&#39;Generation&#39;]) . /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe12e5199e8&gt; . It appears that UMAP in this situation does a much worse job at finding the underlying pattern of the data. . Let&#39;s give NMF a shot . NMF . Very similar to PCA! However, since NMF only takes positive values, we need to rescale it using the MinMaxScaler . ##import Decomposition: NMF, ##Metrics: explained_variance_score ##Preprocessing: MinMaxScaler . Scale that bish! . scaler_min_max = MinMaxScaler() . minmax_pokemon = scaler_min_max.fit_transform(pokemon_num) # scale data to range from 0 to 1. Very useful since NMF only takes positive values . for i in range(8): #We use 8 here since there are 8 features (columns) sns.distplot(minmax_pokemon[:,i],hist=False) . /usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). /usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). /usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). /usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). /usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). /usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). /usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). /usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). . Perform NMF on that nig! . nmf_model = nmf.NMF(n_components=4) . nmf_pokemon = nmf_model.fit_transform(minmax_pokemon) . nmf_shape = nmf_pokemon.shape . def get_score(model, data, scorer=mt.explained_variance_score): &quot;&quot;&quot;Estimate performance of the model on the data&quot;&quot;&quot; model = model.fit(data) prediction = model.inverse_transform(model.transform(data)) #Transform data, then ... return scorer(data, prediction) # Will show how much of variance the model can recreate . PCA_score = get_score(pca,pokemon_num) . UMAP_score = get_score(umap_model, pokemon_num) . NMF_score = get_score(nmf_model, pokemon_num) . print(&#39;Shape of PCA: &#39;, pca_shape) print(&#39;Shape of UMAP: &#39;, umap_shape) print(&#39;Shape of NMF: &#39;, nmf_shape) print (&#39; &#39;) print(&#39;PCA Score &#39;,PCA_score) print(&#39;UMAP Score &#39;,UMAP_score) print(&#39;NMF Score &#39;,NMF_score) . fig = px.scatter_matrix( nmf_reduced_pokemon, #data to plot labels=pca_labels, dimensions=range(4), #We only have 4 components color=pokemon[&quot;Generation&quot;], title = &#39;NMF on numeric types, colored by Generation&#39; #found it interesting to color by Generation, as these may introduce different stats ) fig.update_traces(diagonal_visible=False) fig.show() . fig = px.scatter_matrix( nmf_reduced_pokemon, #data to plot labels=pca_labels, dimensions=range(4), #We only have 4 components color=pokemon[&quot;Legendary_Status&quot;], title = &#39;NMF on numeric types, colored by Generation&#39; #found it interesting to color by Generation, as these may introduce different stats ) fig.update_traces(diagonal_visible=False) fig.show() .",
            "url": "https://achnito.github.io/DSBA/fastpages/jupyter/2020/11/24/M1_Assignment.html",
            "relUrl": "/fastpages/jupyter/2020/11/24/M1_Assignment.html",
            "date": " • Nov 24, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Data Science for Business Applications",
            "content": ". The above code is not visible for the reader. . delete this text box . . . Introduction . In this project, we have chosen to work with an open dataset provided by Yelp. You can download and read more about the data set here: Yelp Dataset. . Yelp is an online review platform that enables people to find user recommendations of a wide range of businesses - and write reviews of their own experiences. To give a review as user is asked to give a star rating on a scale from 1 to 5 and include a description of the experience the user had with the entity. &lt;/br&gt;&lt;/br&gt; . A Breif explanation of the data . This dataset was made available for use in personal, educational and academic purposes. It is a subset consisting of reviews, businesses and users across 10 metropolitan areas. More specifically, it contains data on more than 8 million reviews, two-hundred-thousand businesses, and almost 2 million users. The dataset is downloaded as a compressed tar-file format. Uncompressed there are 5 json-files and a total of 9.8GB data - a rather huge dataset. . On a Data Science stand point this dataset is rather interesting as it contains a vast amount of data, including structured and unstructured data (text). For the more demanding project, Yelp has made an additional dataset available containing 200.000 photos that has been posted on their platform in connection with the reviews. These photos however, are not included in this project. . Description of the Datasets . A description of the dataset is listed in the documentation of the dataset. . Here, we provide you with a simplified description of the data in the datasets: . Business.json . Contains bussiness data including location data, attributes and categories: . business_id: ID of the business | name: name of the business | address: address of the business | city: city of the business | state: state of the business | postal_code: postal code of the business | latitude: latitude of the business | longitude: longitude of the business | stars: average rating of the business | review_count: number of reviews received | is_open: 1 if the business is open, 0 otherwise | attributes: | categories: multiple categories of the business | hours: business opening hours | . Review.json . Contains full review text data including the user_id that wrote the review and the business_id the review was written for: . review_id: ID of the review | user_id: ID of the user | business_id: ID of the business | stars: stars given in review | date: time of review | text: the review | useful: number of people that found the review useful | funny: number of people that found the review funny | cool: number of people that found the review cool | . User.json . Contains user data including the user&#39;s friend mapping and all the metadata associated with the user. . user_id: ID of the review | name: name of the user | review_count: number of reviews given | yelping_since: user creation date | friends: the user’s friends as an array of user_ids | useful: number of useful vores sent by the user | funny: number of funny vores sent by the user | cool: number of cool vores sent by the user | fans: number of fans the user has | elite: years the user had elite status as an array. | average_stars: average star rating given by the user | compliments: number of times the user was complimented. | . After a thorough inspection of the documentation, we decided to use three main data sets: Business.json, review.json and user.json. . These datasets contained features that we found interesting for our project as thought we could use to create our own data set. The business.json contains information about the different businesses, what kind of business they conduct, where they are located and their respective average ratings. . Review.json contains the actual review of the respective business as well the corresponding “review star”. Lastly, user.json contains information about the user who provided the review. Some of the useful columns that we thought would use are the average review score of the user, the “usefulness” of the user (an integer value, that represents how many other “yelpers” thought that this user provided valuable feedback) and the users friend list. We intended to conduct a network analysis based on the information that we have from user.json and the average review score of the users proved to be of value, as we could use this to clean the data set from “extreme” behaviour (people who are “notorious complainers” or from users who uprated a friend’s restaurant). Moreover, we planned to use the reviews (text) from review.json to experiment with Topic Modeling as well as to create a SML model that predicts the sentiment of the reviews. Despite the fact that the reviews have stars, we thought this would be a great opportunity to practice our skills. Lastly, the business.json file was intended to be used to learn facts about businesses. User_id and business_id columns were used to merge the data sources coming from different json files. Limitation: We only read in 1,000,000 lines from the review.json as well as from the user.json. Those 2 data sets were rather large, and our computers lacked the necessary memory to work with them. What this means is that we “lost” some information, however we thought that this would not interfere with our ability to demonstrate our proficiency of the newly acquired methods and tools. (Data preprocessing, EDA, Topic Modeling, SML, etc.) Eventually, our “final” data set (csv) reached a size of 1 GB, thus we had plenty of data to work with. . Installation &amp; Libraries . #Imports import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline from google.colab import files #Use to upload single files from local directory from google.colab import drive #Use to mount google drive to access documents saved in the cloud . Get Data . drive.mount(&#39;/content/drive&#39;) #Mounts Google Drive that holds the data. . Mounted at /content/drive . . Create data set top categories.ipynb . Create a CSV file . The original file is downloaded from Yelp and we create a subset that contains the useful data. . business_json_path = &#39;D:/projects/NLP project DS/yelp_academic_dataset_business.json&#39; business = pd.read_json(business_json_path, lines=True) . business.head() . business_id name address city state postal_code latitude longitude stars review_count is_open attributes categories hours . 0 f9NumwFMBDn751xgFiRbNA | The Range At Lake Norman | 10913 Bailey Rd | Cornelius | NC | 28031 | 35.462724 | -80.852612 | 3.5 | 36 | 1 | {&#39;BusinessAcceptsCreditCards&#39;: &#39;True&#39;, &#39;BikePa... | Active Life, Gun/Rifle Ranges, Guns &amp; Ammo, Sh... | {&#39;Monday&#39;: &#39;10:0-18:0&#39;, &#39;Tuesday&#39;: &#39;11:0-20:0&#39;... | . 1 Yzvjg0SayhoZgCljUJRF9Q | Carlos Santo, NMD | 8880 E Via Linda, Ste 107 | Scottsdale | AZ | 85258 | 33.569404 | -111.890264 | 5.0 | 4 | 1 | {&#39;GoodForKids&#39;: &#39;True&#39;, &#39;ByAppointmentOnly&#39;: &#39;... | Health &amp; Medical, Fitness &amp; Instruction, Yoga,... | None | . 2 XNoUzKckATkOD1hP6vghZg | Felinus | 3554 Rue Notre-Dame O | Montreal | QC | H4C 1P4 | 45.479984 | -73.580070 | 5.0 | 5 | 1 | None | Pets, Pet Services, Pet Groomers | None | . 3 6OAZjbxqM5ol29BuHsil3w | Nevada House of Hose | 1015 Sharp Cir | North Las Vegas | NV | 89030 | 36.219728 | -115.127725 | 2.5 | 3 | 0 | {&#39;BusinessAcceptsCreditCards&#39;: &#39;True&#39;, &#39;ByAppo... | Hardware Stores, Home Services, Building Suppl... | {&#39;Monday&#39;: &#39;7:0-16:0&#39;, &#39;Tuesday&#39;: &#39;7:0-16:0&#39;, ... | . 4 51M2Kk903DFYI6gnB5I6SQ | USE MY GUY SERVICES LLC | 4827 E Downing Cir | Mesa | AZ | 85205 | 33.428065 | -111.726648 | 4.5 | 26 | 1 | {&#39;BusinessAcceptsCreditCards&#39;: &#39;True&#39;, &#39;ByAppo... | Home Services, Plumbing, Electricians, Handyma... | {&#39;Monday&#39;: &#39;0:0-0:0&#39;, &#39;Tuesday&#39;: &#39;9:0-16:0&#39;, &#39;... | . drop_columns = [&#39;address&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;hours&#39;] business = business.drop(drop_columns, axis= 1) . Show business into categories . business.categories.sample(10) . 46949 Nightlife, Bars, Whiskey Bars, Beer, Wine &amp; Sp... 89731 Windshield Installation &amp; Repair, Automotive, ... 66695 Hair Salons, Beauty &amp; Spas 99255 Professional Services, Lawyers, Financial Serv... 171605 Nail Salons, Massage, Health &amp; Medical, Cosmet... 171992 Fashion, Used, Vintage &amp; Consignment, Shopping... 79846 Salad, Ramen, Japanese, Asian Fusion, Restaurants 140752 Adult Entertainment, Counseling &amp; Mental Healt... 123696 Chicken Wings, Pizza, Italian, Restaurants 149195 Outlet Stores, Shopping Name: categories, dtype: object . Look for businesses where category contains the word Nightlife or Bar. . business_nightlife = business[business[&#39;categories&#39;].str.contains(&#39;Nightlife|Bar&#39;, case= False, na= False)] . business_explode = business.assign(categories = business.categories.str.split(&#39;, &#39;)).explode(&#39;categories&#39;) . print(&#39;The top 10 categories:&#39;) business_explode.categories.value_counts()[:10] . The top 10 categories: . Restaurants 63944 Shopping 34644 Food 32991 Home Services 22487 Beauty &amp; Spas 20520 Health &amp; Medical 19227 Local Services 15783 Automotive 14720 Nightlife 14211 Bars 12400 Name: categories, dtype: int64 . visuals = business_explode.categories.value_counts()[:10] . vis = pd.DataFrame(visuals) . vis . categories . Restaurants 63944 | . Shopping 34644 | . Food 32991 | . Home Services 22487 | . Beauty &amp; Spas 20520 | . Health &amp; Medical 19227 | . Local Services 15783 | . Automotive 14720 | . Nightlife 14211 | . Bars 12400 | . plt.figure(figsize= (25, 15)) plt.style.use(&#39;ggplot&#39;) sns.barplot(vis.categories, vis.index) plt.title(&#39;Top 10 categories of businesses&#39;, fontdict= {&#39;fontsize&#39;: 24}) plt.xlabel(&#39;Number of businesses&#39;, fontdict= {&#39;fontsize&#39;: 18}) plt.tick_params(labelsize= 16) plt.savefig(&#39;categories.png&#39;) . Reade in a chunk of reviews from the original dataset. . reviews_json_path = &#39;D:/projects/NLP project DS/yelp_academic_dataset_review.json&#39; . size = 1000000 . review = pd.read_json(reviews_json_path, lines=True, dtype={&#39;review_id&#39;:str,&#39;user_id&#39;:str, &#39;business_id&#39;:str,&#39;stars&#39;:int, &#39;date&#39;:str,&#39;text&#39;:str,&#39;useful&#39;:int, &#39;funny&#39;:int,&#39;cool&#39;:int}, chunksize=size) . If you read in a chunk, read_json() return a JsonReader object for iteration. . Merge business and reviews dataset . chunk_list = [] for chunk_review in review: chunk_review = chunk_review.drop([&#39;review_id&#39;,&#39;useful&#39;,&#39;funny&#39;,&#39;cool&#39;], axis= 1) chunk_review = chunk_review.rename(columns= {&#39;stars&#39;: &#39;review_stars&#39;}) chunk_merged = pd.merge(business_nightlife, chunk_review, on= &#39;business_id&#39;, how= &#39;inner&#39;) print(f&quot;{chunk_merged.shape[0]} out of {size:,} related reviews&quot;) chunk_list.append(chunk_merged) . 273589 out of 1,000,000 related reviews 276014 out of 1,000,000 related reviews 264422 out of 1,000,000 related reviews 272956 out of 1,000,000 related reviews 261842 out of 1,000,000 related reviews 259562 out of 1,000,000 related reviews 260244 out of 1,000,000 related reviews 256680 out of 1,000,000 related reviews 4378 out of 1,000,000 related reviews . business_nightlife_review = pd.concat(chunk_list, ignore_index=True, join=&#39;outer&#39;, axis=0) . Check merged dataset . business_nightlife_review.head() . business_id name city state postal_code stars review_count is_open attributes categories user_id review_stars text date . 0 DCsS3SgVFO56F6wRO_ewgA | Missy Donuts &amp; Coffee | Mesa | AZ | 85201 | 2.5 | 7 | 0 | {&#39;BikeParking&#39;: &#39;True&#39;, &#39;BusinessParking&#39;: &#39;{&#39;... | Donuts, Juice Bars &amp; Smoothies, Food, Coffee &amp;... | cSQnJ7JTY78ki5ai57kZ9A | 4 | 9.99 for a dozen raised is a Lil much the cake... | 2015-08-07 15:02:50 | . 1 DCsS3SgVFO56F6wRO_ewgA | Missy Donuts &amp; Coffee | Mesa | AZ | 85201 | 2.5 | 7 | 0 | {&#39;BikeParking&#39;: &#39;True&#39;, &#39;BusinessParking&#39;: &#39;{&#39;... | Donuts, Juice Bars &amp; Smoothies, Food, Coffee &amp;... | 5_CoaRC22jwmuUzZwlRG_g | 1 | Dirty dinning list will not go here again sad ... | 2016-02-13 17:50:23 | . 2 DCsS3SgVFO56F6wRO_ewgA | Missy Donuts &amp; Coffee | Mesa | AZ | 85201 | 2.5 | 7 | 0 | {&#39;BikeParking&#39;: &#39;True&#39;, &#39;BusinessParking&#39;: &#39;{&#39;... | Donuts, Juice Bars &amp; Smoothies, Food, Coffee &amp;... | 5_CoaRC22jwmuUzZwlRG_g | 5 | Just went here for a late night donut fix lol.... | 2015-09-29 06:18:47 | . 3 DCsS3SgVFO56F6wRO_ewgA | Missy Donuts &amp; Coffee | Mesa | AZ | 85201 | 2.5 | 7 | 0 | {&#39;BikeParking&#39;: &#39;True&#39;, &#39;BusinessParking&#39;: &#39;{&#39;... | Donuts, Juice Bars &amp; Smoothies, Food, Coffee &amp;... | c5ebpS7ex6npffT9Nlvqvw | 1 | Would of given 0 stars if possible first impre... | 2015-11-10 18:55:53 | . 4 DCsS3SgVFO56F6wRO_ewgA | Missy Donuts &amp; Coffee | Mesa | AZ | 85201 | 2.5 | 7 | 0 | {&#39;BikeParking&#39;: &#39;True&#39;, &#39;BusinessParking&#39;: &#39;{&#39;... | Donuts, Juice Bars &amp; Smoothies, Food, Coffee &amp;... | EXys-sSmm5auoqs6Jkyh7g | 5 | This is my first Yelp review. We just tried Mi... | 2016-01-25 02:26:47 | . business_nightlife_review.sample(5) . business_id name city state postal_code stars review_count is_open attributes categories user_id review_stars text date . 1256429 pDSl9xdKJCdN4vu8z26cEA | Coyote Ugly Saloon | Las Vegas | NV | 89109 | 2.5 | 344 | 1 | {&#39;Alcohol&#39;: &#39;u&#39;full_bar&#39;&#39;, &#39;OutdoorSeating&#39;: &#39;... | Bars, Nightlife, Adult Entertainment | W4MoCkTR35IIflqBl-SGQw | 5 | They played a good variety of music. The coyot... | 2013-10-28 16:49:18 | . 323651 DK-RfkH_GXFqjEOipl8kOw | Abbey Road Pub &amp; Patio | Mississauga | ON | L5L 1W8 | 3.5 | 23 | 1 | {&#39;RestaurantsGoodForGroups&#39;: &#39;True&#39;, &#39;Business... | Pubs, Bars, Nightlife | RRP7_dqWbnpeQx7LE0x3CA | 3 | Love the front section...Great atmosphere on F... | 2018-02-17 01:28:46 | . 2030367 1aEx-I_JdWPl6ITQDQAJxQ | Village Barber Shop | Henderson | NV | 89052 | 4.0 | 113 | 1 | {&#39;BusinessParking&#39;: &#39;{&#39;garage&#39;: False, &#39;street... | Barbers, Beauty &amp; Spas, Hair Stylists, Hair Sa... | bgJLZAEaB04x9UQIm3sATQ | 5 | Love this place. The folks who run the chairs ... | 2014-06-24 15:40:39 | . 1921867 5j2ugUALtjsa2nkV1YGq5Q | House Modern Sushi Restaurant | Mesa | AZ | 85204 | 4.0 | 311 | 1 | {&#39;RestaurantsReservations&#39;: &#39;True&#39;, &#39;Caters&#39;: ... | Restaurants, Sushi Bars, Japanese, Asian Fusion | 7v7-05j0pi5RDbvNJQB5LA | 2 | You get what you pay for. The prices here are ... | 2014-02-23 01:00:16 | . 448286 HG3ROmv3FJZTMJPpThSXOw | Luke Wholey&#39;s Wild Alaskan Grille | Pittsburgh | PA | 15222 | 4.0 | 470 | 1 | {&#39;RestaurantsPriceRange2&#39;: &#39;2&#39;, &#39;OutdoorSeatin... | Cocktail Bars, Sushi Bars, Bars, Nightlife, Re... | 5NiG9F8yaZ9KIsfk3TGAYQ | 5 | I am a seafood lover.. Not ever terribly picky... | 2014-09-17 23:16:45 | . business_nightlife_review.shape . (2129687, 14) . Save to a CSV file . name = &#39;business_nightlife_review.csv&#39; business_nightlife_review.to_csv(name, index= False) . . Market delimination . Read in data . df = pd.read_csv(&#39;data/nightlife_review.csv&#39;) . df.shape . (2129687, 15) . Look at the top of data set . df.head(10) . business_id name city state postal_code stars review_count is_open attributes categories user_id review_stars text date experience . 0 DCsS3SgVFO56F6wRO_ewgA | Missy Donuts &amp; Coffee | Mesa | AZ | 85201 | 2.5 | 7 | 0 | {&#39;BikeParking&#39;: &#39;True&#39;, &#39;BusinessParking&#39;: &quot;{&#39;... | Donuts, Juice Bars &amp; Smoothies, Food, Coffee &amp;... | cSQnJ7JTY78ki5ai57kZ9A | 4 | 9.99 for a dozen raised is a Lil much the cake... | 2015-08-07 15:02:50 | 0 | . 1 DCsS3SgVFO56F6wRO_ewgA | Missy Donuts &amp; Coffee | Mesa | AZ | 85201 | 2.5 | 7 | 0 | {&#39;BikeParking&#39;: &#39;True&#39;, &#39;BusinessParking&#39;: &quot;{&#39;... | Donuts, Juice Bars &amp; Smoothies, Food, Coffee &amp;... | 5_CoaRC22jwmuUzZwlRG_g | 1 | Dirty dinning list will not go here again sad ... | 2016-02-13 17:50:23 | 0 | . 2 DCsS3SgVFO56F6wRO_ewgA | Missy Donuts &amp; Coffee | Mesa | AZ | 85201 | 2.5 | 7 | 0 | {&#39;BikeParking&#39;: &#39;True&#39;, &#39;BusinessParking&#39;: &quot;{&#39;... | Donuts, Juice Bars &amp; Smoothies, Food, Coffee &amp;... | 5_CoaRC22jwmuUzZwlRG_g | 5 | Just went here for a late night donut fix lol.... | 2015-09-29 06:18:47 | 0 | . 3 DCsS3SgVFO56F6wRO_ewgA | Missy Donuts &amp; Coffee | Mesa | AZ | 85201 | 2.5 | 7 | 0 | {&#39;BikeParking&#39;: &#39;True&#39;, &#39;BusinessParking&#39;: &quot;{&#39;... | Donuts, Juice Bars &amp; Smoothies, Food, Coffee &amp;... | c5ebpS7ex6npffT9Nlvqvw | 1 | Would of given 0 stars if possible first impre... | 2015-11-10 18:55:53 | 0 | . 4 DCsS3SgVFO56F6wRO_ewgA | Missy Donuts &amp; Coffee | Mesa | AZ | 85201 | 2.5 | 7 | 0 | {&#39;BikeParking&#39;: &#39;True&#39;, &#39;BusinessParking&#39;: &quot;{&#39;... | Donuts, Juice Bars &amp; Smoothies, Food, Coffee &amp;... | EXys-sSmm5auoqs6Jkyh7g | 5 | This is my first Yelp review. We just tried Mi... | 2016-01-25 02:26:47 | 0 | . 5 DCsS3SgVFO56F6wRO_ewgA | Missy Donuts &amp; Coffee | Mesa | AZ | 85201 | 2.5 | 7 | 0 | {&#39;BikeParking&#39;: &#39;True&#39;, &#39;BusinessParking&#39;: &quot;{&#39;... | Donuts, Juice Bars &amp; Smoothies, Food, Coffee &amp;... | TPXPy309BrtLXbRorw3WKg | 2 | The second star is for the delicious apple fri... | 2015-09-19 18:22:09 | 0 | . 6 DCsS3SgVFO56F6wRO_ewgA | Missy Donuts &amp; Coffee | Mesa | AZ | 85201 | 2.5 | 7 | 0 | {&#39;BikeParking&#39;: &#39;True&#39;, &#39;BusinessParking&#39;: &quot;{&#39;... | Donuts, Juice Bars &amp; Smoothies, Food, Coffee &amp;... | PNa8MaoGmptKej1K3hPVow | 2 | Ok service &amp; donuts. Bad boba(boba was hard an... | 2016-03-05 20:45:48 | 0 | . 7 DCsS3SgVFO56F6wRO_ewgA | Missy Donuts &amp; Coffee | Mesa | AZ | 85201 | 2.5 | 7 | 0 | {&#39;BikeParking&#39;: &#39;True&#39;, &#39;BusinessParking&#39;: &quot;{&#39;... | Donuts, Juice Bars &amp; Smoothies, Food, Coffee &amp;... | YEg0bICRG3o_MSNqbcIZaw | 2 | Update: Wow. I just read through Missy&#39;s healt... | 2016-03-08 15:43:15 | 0 | . 8 Mmd5WDFq9hHcQ3uClngGjQ | Irene&#39;s Tap Room | Phoenix | AZ | 85020 | 4.5 | 79 | 1 | {&#39;WiFi&#39;: &quot;u&#39;free&#39;&quot;, &#39;NoiseLevel&#39;: &quot;u&#39;average&#39;&quot;... | Bars, Arts &amp; Entertainment, Nightlife, Music V... | 3i095Fnh08yC-Q3VY0KeHQ | 5 | Our new favorite local spot. The owners are fa... | 2018-08-30 01:39:54 | 1 | . 9 Mmd5WDFq9hHcQ3uClngGjQ | Irene&#39;s Tap Room | Phoenix | AZ | 85020 | 4.5 | 79 | 1 | {&#39;WiFi&#39;: &quot;u&#39;free&#39;&quot;, &#39;NoiseLevel&#39;: &quot;u&#39;average&#39;&quot;... | Bars, Arts &amp; Entertainment, Nightlife, Music V... | MeLrLNf-aoz9niUmRMmb9g | 5 | Stopped in at Irene&#39;s after work. This place ... | 2018-08-21 00:48:47 | 1 | . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2129687 entries, 0 to 2129686 Data columns (total 15 columns): # Column Dtype -- 0 business_id object 1 name object 2 city object 3 state object 4 postal_code object 5 stars float64 6 review_count int64 7 is_open int64 8 attributes object 9 categories object 10 user_id object 11 review_stars int64 12 text object 13 date object 14 experience int64 dtypes: float64(1), int64(4), object(10) memory usage: 243.7+ MB . Create supply and demand proxies &amp; pandas data frame . supply = pd.DataFrame(df.groupby(&#39;city&#39;).name.nunique().sort_values(ascending= False)) . supply.columns = [&#39;supply&#39;] . supply.head() . supply . city . Las Vegas 3080 | . Toronto 2755 | . Phoenix 1449 | . Montréal 1166 | . Charlotte 1093 | . demand = pd.DataFrame(df.groupby(&#39;city&#39;).size().sort_values(ascending= False)) . demand.columns = [&#39;demand&#39;] . demand.head() . demand . city . Las Vegas 727107 | . Phoenix 211114 | . Toronto 156352 | . Scottsdale 141971 | . Charlotte 120033 | . df_dem_sup = supply.merge(demand, on= &#39;city&#39;) . df_dem_sup.head() . supply demand . city . Las Vegas 3080 | 727107 | . Toronto 2755 | 156352 | . Phoenix 1449 | 211114 | . Montréal 1166 | 38870 | . Charlotte 1093 | 120033 | . Calculate market saturation and save it in column . df_dem_sup[&#39;market_saturation&#39;] = df_dem_sup.supply / df_dem_sup.demand * 100 . Sort the dataframe entries by market saturation (ascending) . df_dem_sup = df_dem_sup[(df_dem_sup.demand &gt; 5000)] . df_dem_sup = df_dem_sup.sort_values(by= &#39;market_saturation&#39;) . df_dem_sup.shape . (28, 3) . df_dem_sup.head(20) . supply demand market_saturation . city . Las Vegas 3080 | 727107 | 0.423597 | . Henderson 328 | 53926 | 0.608241 | . Scottsdale 866 | 141971 | 0.609984 | . Gilbert 208 | 31056 | 0.669758 | . Phoenix 1449 | 211114 | 0.686359 | . Chandler 337 | 47316 | 0.712233 | . Goodyear 67 | 9083 | 0.737642 | . Tempe 418 | 55787 | 0.749279 | . Peoria 137 | 16001 | 0.856196 | . Charlotte 1093 | 120033 | 0.910583 | . Surprise 84 | 8546 | 0.982916 | . North Las Vegas 102 | 10260 | 0.994152 | . Huntersville 61 | 6004 | 1.015989 | . Mesa 367 | 35609 | 1.030638 | . Glendale 286 | 26991 | 1.059612 | . Pittsburgh 985 | 82639 | 1.191931 | . Madison 482 | 37497 | 1.285436 | . Cleveland 637 | 48335 | 1.317886 | . Lakewood 114 | 8371 | 1.361844 | . Markham 191 | 12081 | 1.580995 | . Create visual summary of market saturation . plt.figure(figsize= (25, 15)) plt.title(&#39;20 LOWEST SATURATED MARKETS (LOWER IS BETTER)&#39;, fontdict= {&#39;fontsize&#39;: 24}) sns.barplot(df_dem_sup.market_saturation[:20], df_dem_sup.index[:20]) plt.ylabel(&#39;cities&#39;, fontdict= {&#39;fontsize&#39;: 18}) plt.xlabel(&#39;market saturation&#39;, rotation= 0, fontdict= {&#39;fontsize&#39;: 18}) plt.tick_params(labelsize= 16) plt.savefig(&#39;market_saturation.png&#39;) . Phoenix is the chosen city, as the market saturation is relatively low and there is a rather high demand present in the city. . phoenix = df[(df.city == &#39;Phoenix&#39;)] phoenix.head() . business_id name city state postal_code stars review_count is_open attributes categories user_id review_stars text date experience . 8 Mmd5WDFq9hHcQ3uClngGjQ | Irene&#39;s Tap Room | Phoenix | AZ | 85020 | 4.5 | 79 | 1 | {&#39;WiFi&#39;: &quot;u&#39;free&#39;&quot;, &#39;NoiseLevel&#39;: &quot;u&#39;average&#39;&quot;... | Bars, Arts &amp; Entertainment, Nightlife, Music V... | 3i095Fnh08yC-Q3VY0KeHQ | 5 | Our new favorite local spot. The owners are fa... | 2018-08-30 01:39:54 | 1 | . 9 Mmd5WDFq9hHcQ3uClngGjQ | Irene&#39;s Tap Room | Phoenix | AZ | 85020 | 4.5 | 79 | 1 | {&#39;WiFi&#39;: &quot;u&#39;free&#39;&quot;, &#39;NoiseLevel&#39;: &quot;u&#39;average&#39;&quot;... | Bars, Arts &amp; Entertainment, Nightlife, Music V... | MeLrLNf-aoz9niUmRMmb9g | 5 | Stopped in at Irene&#39;s after work. This place ... | 2018-08-21 00:48:47 | 1 | . 10 Mmd5WDFq9hHcQ3uClngGjQ | Irene&#39;s Tap Room | Phoenix | AZ | 85020 | 4.5 | 79 | 1 | {&#39;WiFi&#39;: &quot;u&#39;free&#39;&quot;, &#39;NoiseLevel&#39;: &quot;u&#39;average&#39;&quot;... | Bars, Arts &amp; Entertainment, Nightlife, Music V... | E9IJMZ2njmG1LOMmOhzeBg | 5 | Such a fun atmosphere!! Went there last night ... | 2019-01-12 21:25:24 | 1 | . 11 Mmd5WDFq9hHcQ3uClngGjQ | Irene&#39;s Tap Room | Phoenix | AZ | 85020 | 4.5 | 79 | 1 | {&#39;WiFi&#39;: &quot;u&#39;free&#39;&quot;, &#39;NoiseLevel&#39;: &quot;u&#39;average&#39;&quot;... | Bars, Arts &amp; Entertainment, Nightlife, Music V... | 7KZhUr0i3K202Ux1-OGOhQ | 4 | This place is new and very close to us, so we ... | 2018-07-30 01:57:23 | 1 | . 12 Mmd5WDFq9hHcQ3uClngGjQ | Irene&#39;s Tap Room | Phoenix | AZ | 85020 | 4.5 | 79 | 1 | {&#39;WiFi&#39;: &quot;u&#39;free&#39;&quot;, &#39;NoiseLevel&#39;: &quot;u&#39;average&#39;&quot;... | Bars, Arts &amp; Entertainment, Nightlife, Music V... | UIKrJGEyTCWKaAvWDhLJug | 5 | This is exactly what this neighborhood needed.... | 2018-07-23 02:01:32 | 1 | . phoenix.shape . (211114, 15) . phoenix.isnull().sum() . business_id 0 name 0 city 0 state 0 postal_code 7 stars 0 review_count 0 is_open 0 attributes 106 categories 0 user_id 0 review_stars 0 text 0 date 0 experience 0 dtype: int64 . Save data set to CSV . name = &#39;data/phoenix_nightlife.csv&#39; phoenix.to_csv(name, index= False) . . Merge users phoenix. . Import JSON in chunk . path = &#39;data/yelp_academic_dataset_user.json&#39; . size = 1000000 . users = pd.read_json(path, lines= True, dtype= {&#39;user_id&#39;: str, &#39;name&#39;: str, &#39;review_count&#39;: int, &#39;yelping_since&#39;: str, &#39;friends&#39;: list, &#39;useful&#39;: int, &#39;funny&#39;: int, &#39;cool&#39;: int, &#39;fans&#39;: int, &#39;elite&#39;: list, &#39;avarage_stars&#39;: float, &#39;compliment_hot&#39;: int, &#39;comliment_more&#39;: int, &#39;compliment_profile&#39;: 42, &#39;compliment_cute&#39;: int, &#39;comliment_list&#39;: int, &#39;compliment_note&#39;: int, &#39;comliment_plain&#39;: int, &#39;compliment_cool&#39;: int, &#39;compliment_funny&#39;: int, &#39;compliment_writer&#39;: int, &#39;compliment_photos&#39;: int}, chunksize= size) . Import phoenix data set . phoenix = pd.read_csv(&#39;data/phoenix_nightlife.csv&#39;) . phoenix.shape . (211114, 15) . phoenix.head(3) . business_id name city state postal_code stars review_count is_open attributes categories user_id review_stars text date experience . 0 Mmd5WDFq9hHcQ3uClngGjQ | Irene&#39;s Tap Room | Phoenix | AZ | 85020.0 | 4.5 | 79 | 1 | {&#39;WiFi&#39;: &quot;u&#39;free&#39;&quot;, &#39;NoiseLevel&#39;: &quot;u&#39;average&#39;&quot;... | Bars, Arts &amp; Entertainment, Nightlife, Music V... | 3i095Fnh08yC-Q3VY0KeHQ | 5 | Our new favorite local spot. The owners are fa... | 2018-08-30 01:39:54 | 1 | . 1 Mmd5WDFq9hHcQ3uClngGjQ | Irene&#39;s Tap Room | Phoenix | AZ | 85020.0 | 4.5 | 79 | 1 | {&#39;WiFi&#39;: &quot;u&#39;free&#39;&quot;, &#39;NoiseLevel&#39;: &quot;u&#39;average&#39;&quot;... | Bars, Arts &amp; Entertainment, Nightlife, Music V... | MeLrLNf-aoz9niUmRMmb9g | 5 | Stopped in at Irene&#39;s after work. This place ... | 2018-08-21 00:48:47 | 1 | . 2 Mmd5WDFq9hHcQ3uClngGjQ | Irene&#39;s Tap Room | Phoenix | AZ | 85020.0 | 4.5 | 79 | 1 | {&#39;WiFi&#39;: &quot;u&#39;free&#39;&quot;, &#39;NoiseLevel&#39;: &quot;u&#39;average&#39;&quot;... | Bars, Arts &amp; Entertainment, Nightlife, Music V... | E9IJMZ2njmG1LOMmOhzeBg | 5 | Such a fun atmosphere!! Went there last night ... | 2019-01-12 21:25:24 | 1 | . Iterate through users and merge with phoenix data set . Rename review_count column in users. Review count column now refers to number of reviews for the specific business and num_review_written refers to the number of reviews written by that user. . chunk_list = [] for chunk_user in users: chunk_user = chunk_user.drop([&#39;name&#39;, &#39;yelping_since&#39;, &#39;funny&#39;, &#39;cool&#39;, &#39;fans&#39;, &#39;elite&#39;, &#39;compliment_hot&#39;, &#39;compliment_more&#39;, &#39;compliment_profile&#39;, &#39;compliment_cute&#39;, &#39;compliment_list&#39;, &#39;compliment_note&#39;, &#39;compliment_plain&#39;, &#39;compliment_cool&#39;, &#39;compliment_funny&#39;, &#39;compliment_writer&#39;, &#39;compliment_photos&#39;], axis= 1) chunk_user = chunk_user.rename(columns= {&#39;review_count&#39;: &#39;num_reviews_written&#39;}) chunk_merged = pd.merge(phoenix, chunk_user, on= &#39;user_id&#39;, how= &#39;inner&#39;) print(f&quot;{chunk_merged.shape[0]} out of {size:,} related users.&quot;) chunk_list.append(chunk_merged) . 180426 out of 1,000,000 related users. 30688 out of 1,000,000 related users. . Concatenate data frame &quot;pieces&quot; into one data frame. . phoenix_nightlife_user = pd.concat(chunk_list, ignore_index= True, join= &#39;outer&#39;, axis= 0) . phoenix_nightlife_user.shape . (211114, 19) . phoenix_nightlife_user.head(3) . business_id name city state postal_code stars review_count is_open attributes categories user_id review_stars text date experience num_reviews_written useful friends average_stars . 0 Mmd5WDFq9hHcQ3uClngGjQ | Irene&#39;s Tap Room | Phoenix | AZ | 85020.0 | 4.5 | 79 | 1 | {&#39;WiFi&#39;: &quot;u&#39;free&#39;&quot;, &#39;NoiseLevel&#39;: &quot;u&#39;average&#39;&quot;... | Bars, Arts &amp; Entertainment, Nightlife, Music V... | 3i095Fnh08yC-Q3VY0KeHQ | 5 | Our new favorite local spot. The owners are fa... | 2018-08-30 01:39:54 | 1 | 5 | 1 | qQ61hXIe8U8jPXBf4q81WA, 2qmYYG0qoa7RGxW4EvRwdA... | 4.2 | . 1 ku9ak9cQnIGBCXBtRd61tQ | Spoke &amp; Wheel - Phoenix | Phoenix | AZ | 85020.0 | 4.0 | 357 | 1 | {&#39;NoiseLevel&#39;: &quot;u&#39;average&#39;&quot;, &#39;WiFi&#39;: &quot;u&#39;free&#39;&quot;... | Bars, Beer, Wine &amp; Spirits, Sandwiches, Breakf... | 3i095Fnh08yC-Q3VY0KeHQ | 5 | This is our favorite local spot! The wings are... | 2018-08-30 01:37:47 | 1 | 5 | 1 | qQ61hXIe8U8jPXBf4q81WA, 2qmYYG0qoa7RGxW4EvRwdA... | 4.2 | . 2 Mmd5WDFq9hHcQ3uClngGjQ | Irene&#39;s Tap Room | Phoenix | AZ | 85020.0 | 4.5 | 79 | 1 | {&#39;WiFi&#39;: &quot;u&#39;free&#39;&quot;, &#39;NoiseLevel&#39;: &quot;u&#39;average&#39;&quot;... | Bars, Arts &amp; Entertainment, Nightlife, Music V... | MeLrLNf-aoz9niUmRMmb9g | 5 | Stopped in at Irene&#39;s after work. This place ... | 2018-08-21 00:48:47 | 1 | 12 | 4 | p12DAvNJZ5szp8jP69NUbA, JdJ0b9QZKVrjv_nicDhABw... | 5.0 | . Create experience column, based on review star. . phoenix_nightlife_user = phoenix_nightlife_user.drop(&#39;experience&#39;, axis= 1) phoenix_nightlife_user.shape . (211114, 18) . Positive experience = 1, Negative experience = 0 . mappings = {1: 0, 2: 0, 3: 0, 4: 1, 5: 1} phoenix_nightlife_user[&#39;experience&#39;] = phoenix_nightlife_user.review_stars.map(mappings) phoenix_nightlife_user.experience.sample(5) . 27665 1 90276 1 5348 1 131733 1 160394 0 Name: experience, dtype: int64 . Visualize distribution of columns before removing outliers . plt.title(&#39;Distribution of avarage star ratings of users&#39;) plt.hist(phoenix_nightlife_user.average_stars); . plt.title(&#39;Distribution of number of reviews written per user&#39;) plt.yscale(&#39;log&#39;) plt.hist(phoenix_nightlife_user.num_reviews_written); . plt.title(&#39;Distribution of number of reviews per business&#39;) plt.hist(phoenix_nightlife_user.review_count); . Remove outliers . IQR = 75th percentile - 25th percentile (Q3 - Q1). Outlier (left tail) = 25th percentile - IQR 1.5. Outlier (right tail) = 75th percentile + IQR 1.5. . Calculate IQR for the columns we intend to &quot;clean&quot;. . IQR_stars = phoenix_nightlife_user.average_stars.quantile(0.75) - phoenix_nightlife_user.average_stars.quantile(0.25) IQR_num_rev_wri = phoenix_nightlife_user.num_reviews_written.quantile(0.75) - phoenix_nightlife_user.num_reviews_written.quantile(0.25) IQR_rev_count = phoenix_nightlife_user.review_count.quantile(0.75) - phoenix_nightlife_user.review_count.quantile(0.25) . Calculate thresholds for 3 variables to classify outliers. . outlier_stars = IQR_stars * 1.5 outlier_num_rev_wri = IQR_num_rev_wri * 1.5 outlier_rev_count = IQR_rev_count * 1.5 . Calculate Q1 and Q3 for 3 variables . Q1_stars = phoenix_nightlife_user.average_stars.quantile(0.25) Q3_stars = phoenix_nightlife_user.average_stars.quantile(0.75) Q1_num = phoenix_nightlife_user.num_reviews_written.quantile(0.25) Q3_num = phoenix_nightlife_user.num_reviews_written.quantile(0.75) Q1_rev = phoenix_nightlife_user.review_count.quantile(0.25) Q3_rev = phoenix_nightlife_user.review_count.quantile(0.75) . Create final data set, that contains businesses in Phoenix and their respective reviews, along with useful information about the user who gave said review. &quot;Outliers&quot; are removed from data set. . phoenix_nightlife_user = phoenix_nightlife_user[(phoenix_nightlife_user.average_stars &gt;= Q1_stars - outlier_stars) | (phoenix_nightlife_user.average_stars &lt;= Q3_stars + outlier_stars) &amp; (phoenix_nightlife_user.num_reviews_written &gt;= Q1_num - outlier_num_rev_wri) | (phoenix_nightlife_user.num_reviews_written &lt;= Q3_num + outlier_num_rev_wri) &amp; (phoenix_nightlife_user.review_count &gt;= Q1_rev - outlier_rev_count) | (phoenix_nightlife_user.review_count &lt;= Q3_rev + outlier_rev_count)] . phoenix_nightlife_user.shape . (211114, 19) . name = &#39;data/phoenix_users.csv&#39; phoenix_nightlife_user.to_csv(name, index= False) . Visualize distribution of columns after removing outliers . plt.title(&#39;Distribution of avarage star ratings of users&#39;) plt.hist(phoenix_nightlife_user.average_stars); . plt.title(&#39;Distribution of number of reviews written per user&#39;) plt.yscale(&#39;log&#39;) plt.hist(phoenix_nightlife_user.num_reviews_written); . plt.title(&#39;Distribution of number of reviews per business&#39;) plt.hist(phoenix_nightlife_user.review_count); . . Data Exploration . Define data paths . path = &#39;/content/drive/MyDrive/DSBA_Project/yelp_dataset/phoenix_nightlife.csv&#39; . Load DataFrame . df = pd.read_csv(path) . df.head() . business_id name city state postal_code stars review_count is_open attributes categories user_id review_stars text date experience . 0 Mmd5WDFq9hHcQ3uClngGjQ | Irene&#39;s Tap Room | Phoenix | AZ | 85020.0 | 4.5 | 79 | 1 | {&#39;WiFi&#39;: &quot;u&#39;free&#39;&quot;, &#39;NoiseLevel&#39;: &quot;u&#39;average&#39;&quot;... | Bars, Arts &amp; Entertainment, Nightlife, Music V... | 3i095Fnh08yC-Q3VY0KeHQ | 5 | Our new favorite local spot. The owners are fa... | 2018-08-30 01:39:54 | 1 | . 1 Mmd5WDFq9hHcQ3uClngGjQ | Irene&#39;s Tap Room | Phoenix | AZ | 85020.0 | 4.5 | 79 | 1 | {&#39;WiFi&#39;: &quot;u&#39;free&#39;&quot;, &#39;NoiseLevel&#39;: &quot;u&#39;average&#39;&quot;... | Bars, Arts &amp; Entertainment, Nightlife, Music V... | MeLrLNf-aoz9niUmRMmb9g | 5 | Stopped in at Irene&#39;s after work. This place ... | 2018-08-21 00:48:47 | 1 | . 2 Mmd5WDFq9hHcQ3uClngGjQ | Irene&#39;s Tap Room | Phoenix | AZ | 85020.0 | 4.5 | 79 | 1 | {&#39;WiFi&#39;: &quot;u&#39;free&#39;&quot;, &#39;NoiseLevel&#39;: &quot;u&#39;average&#39;&quot;... | Bars, Arts &amp; Entertainment, Nightlife, Music V... | E9IJMZ2njmG1LOMmOhzeBg | 5 | Such a fun atmosphere!! Went there last night ... | 2019-01-12 21:25:24 | 1 | . 3 Mmd5WDFq9hHcQ3uClngGjQ | Irene&#39;s Tap Room | Phoenix | AZ | 85020.0 | 4.5 | 79 | 1 | {&#39;WiFi&#39;: &quot;u&#39;free&#39;&quot;, &#39;NoiseLevel&#39;: &quot;u&#39;average&#39;&quot;... | Bars, Arts &amp; Entertainment, Nightlife, Music V... | 7KZhUr0i3K202Ux1-OGOhQ | 4 | This place is new and very close to us, so we ... | 2018-07-30 01:57:23 | 1 | . 4 Mmd5WDFq9hHcQ3uClngGjQ | Irene&#39;s Tap Room | Phoenix | AZ | 85020.0 | 4.5 | 79 | 1 | {&#39;WiFi&#39;: &quot;u&#39;free&#39;&quot;, &#39;NoiseLevel&#39;: &quot;u&#39;average&#39;&quot;... | Bars, Arts &amp; Entertainment, Nightlife, Music V... | UIKrJGEyTCWKaAvWDhLJug | 5 | This is exactly what this neighborhood needed.... | 2018-07-23 02:01:32 | 1 | . We start of by getting an overview of the positive and negative ratings . star_rating = df.groupby([&#39;business_id&#39;, &#39;name&#39;, &#39;city&#39;, &#39;review_count&#39;])[&#39;stars&#39;].mean() . star_rating = pd.DataFrame(star_rating).sort_values(by=[&#39;stars&#39;, &#39;review_count&#39;], ascending = [False, False]) . star_rating.reset_index(inplace=True) . star_rating.head(20) . index business_id name city review_count stars . 0 845 | Xg5qEQiB-7L6kGJ5F4K3bQ | Little Miss BBQ | Phoenix | 2329 | 5.0 | . 1 1579 | zJ__js7-Cx_wU4GhdBdeRA | The Refuge Coffee &amp; Wine | Phoenix | 163 | 5.0 | . 2 1038 | eZqoDfBg-xOS8WdcZCYM2A | JL Smokehouse | Phoenix | 146 | 5.0 | . 3 612 | O4kYogia0fbF85pCX10XiA | Koi Poke - Arcadia | Phoenix | 144 | 5.0 | . 4 1059 | fHKvkiSHuUYM10Z7KqBrbQ | ILLEET Entertainment | Phoenix | 124 | 5.0 | . 5 170 | 5pEQwYimyfp5wpeudYNahw | El Charro Hipster Bar and Cafe | Phoenix | 122 | 5.0 | . 6 1051 | exwQfCcTmOnMFn94SQZORw | Pure Barre - Phoenix | Phoenix | 82 | 5.0 | . 7 826 | X1-AbThDu9tmijcGnsi8Ug | Binkley&#39;s Restaurant | Phoenix | 79 | 5.0 | . 8 680 | R9GAIXv4DcbgZH-eUd_qIw | Giving Tree Cafe | Phoenix | 67 | 5.0 | . 9 1521 | x0g7c-7tnsMaZtnlZ4CadQ | True North Barber Shop | Phoenix | 52 | 5.0 | . 10 607 | NvdId69PI1Fq9d8EfzxQ0g | Snip Snip | Phoenix | 49 | 5.0 | . 11 326 | COfmsJPeRu_4qFzDpvAAgw | Rewined Beer And Wine Bar | Phoenix | 46 | 5.0 | . 12 835 | XRp0fJNimSJz2ZINgz3nDA | Niza Lideo | Phoenix | 42 | 5.0 | . 13 912 | _TVPXjF1gDXEiWrKAYo1XA | Sidecar Social Club | Phoenix | 33 | 5.0 | . 14 35 | 0P7gYbegHEH0lFkwBoA0og | Front Pourch Brewing | Phoenix | 32 | 5.0 | . 15 1239 | l9uGl3sRTpws4ulfwH3Cmw | Pop&#39;s Barbershop | Phoenix | 31 | 5.0 | . 16 1143 | i8nem7_isYgxPgEs1UV_xg | El Migos WATER-N-ICE | Phoenix | 30 | 5.0 | . 17 279 | AftgPs11yy6D__xM4yr2FQ | The Studio - A Social Fitness Club | Phoenix | 29 | 5.0 | . 18 475 | IVLxwoFx_zXuWJt3cEveAA | Limoz | Phoenix | 29 | 5.0 | . 19 749 | UB-tbeHJk6z9IOEzl_EY3g | Lacuna Kava Bar | Phoenix | 28 | 5.0 | . star_rating_skimmed = star_rating[star_rating.review_count &gt;= 10] . star_rating_skimmed . index business_id name city review_count stars . 0 845 | Xg5qEQiB-7L6kGJ5F4K3bQ | Little Miss BBQ | Phoenix | 2329 | 5.0 | . 1 1579 | zJ__js7-Cx_wU4GhdBdeRA | The Refuge Coffee &amp; Wine | Phoenix | 163 | 5.0 | . 2 1038 | eZqoDfBg-xOS8WdcZCYM2A | JL Smokehouse | Phoenix | 146 | 5.0 | . 3 612 | O4kYogia0fbF85pCX10XiA | Koi Poke - Arcadia | Phoenix | 144 | 5.0 | . 4 1059 | fHKvkiSHuUYM10Z7KqBrbQ | ILLEET Entertainment | Phoenix | 124 | 5.0 | . ... ... | ... | ... | ... | ... | ... | . 1558 203 | 7U7C1xcvuObJqXFp6IWKWQ | J &amp; Gios Barber Shop | Phoenix | 10 | 2.0 | . 1559 799 | W6Do-RqSULOil1LtZ6XwxA | The Original BBQ 2 U | Phoenix | 10 | 2.0 | . 1577 1027 | e69v6hfCkr25M5DaLSS8CQ | Long Wongs | Phoenix | 76 | 1.5 | . 1578 956 | avcKxfAIAImSWv8bKcnGzg | Club Luxx | Phoenix | 58 | 1.5 | . 1579 1093 | g7EXpsDsd-xLQVajvEUnrg | Fruitlicious | Phoenix | 13 | 1.5 | . 1232 rows × 6 columns . sns.barplot(star_rating_skimmed.review_count[:20], star_rating_skimmed.name[:20]) plt.xscale(&#39;log&#39;) . /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . You were here . negative_ratings = len(df[df[&quot;stars&quot;]&lt;2]) positive_ratings = len(df[df[&quot;stars&quot;]&gt;3]) . print(&quot;Total negative reviews: {}&quot;.format(negative_ratings)) print(&quot;Total positive reviews: {}&quot;.format(positive_ratings)) . Total negative reviews: 204 Total positive reviews: 186212 . Let us now get an overview of the top rated businesses in phoenix . top_rated = df[df[&quot;stars&quot;]&gt;3] top_rated_dict ={} for business_id in top_rated[&quot;business_id&quot;].values: try : top_rated_dict[business_id] = top_rated_dict[business_id]=5 except: top_rated_dict[business_id]=0 # topbusiness = pd.DataFrame.from_dict(data= top_rated_dict,orient=&quot;index&quot;) topbusiness.reset_index(inplace=True) topbusiness.columns = [&#39;business_id&#39;, &#39;stars&#39;] del(top_rated_dict) del(top_rated) . top_count= 20 right=pd.DataFrame(df[[&#39;business_id&#39;,&quot;name&quot;,]].values, columns=[&#39;business_id&#39;,&quot;Business name&quot;]) top_business_data = pd.merge(topbusiness,right=right, how=&quot;inner&quot;,on=&#39;business_id&#39;) top_business_data.sort_values(&quot;stars&quot;)[::-1][:top_count].plot(x=&quot;Business name&quot;,y=&quot;stars&quot;, kind=&quot;bar&quot;,figsize=(14,6), title=&#39;Positive reviews&#39;).set_ylabel(&quot;Total ratings&quot;) del(topbusiness) del(right) .",
            "url": "https://achnito.github.io/DSBA/fastpages/jupyter/2020/11/24/Exam_Project.html",
            "relUrl": "/fastpages/jupyter/2020/11/24/Exam_Project.html",
            "date": " • Nov 24, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://achnito.github.io/DSBA/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://achnito.github.io/DSBA/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://achnito.github.io/DSBA/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://achnito.github.io/DSBA/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}